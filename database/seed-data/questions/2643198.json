{
  "id": "/questions/2643198",
  "creationDate": "2014-04-17T14:36:38.380",
  "body": "\n\nI need to determine the screen density at runtime in an Android AppWidget.  I've set up an HDPI emulator device (avd).  If set up a regular executable project, and insert this code into the onCreate method:\n\n\tDisplayMetrics dm = getResources().getDisplayMetrics();\n\tLog.d(\"MyTag\", \"screen density \" + dm.densityDpi);\n\t\n\nThis outputs \"screen density 240\" as expected.\n\nHowever, if I set up an AppWidget project, and insert this code into the onUpdate method:\n\n\tDisplayMetrics dm = context.getResources().getDisplayMetrics();\n\tLog.d(\"MyTag\", \"screen density \" + dm.densityDpi);\n\t\n\nThis outputs \"screen density 160\".  I noticed, hooking up the debugger, that the mDefaultDisplay member of the Resources object here is null in the AppWidget case.\n\nSimilarly, if I get a resource at runtime using the Resources object obtained from context.getResources() in the AppWidget, it returns the wrong resource based on screen density.  For instance, I have a 60x60px drawable for mdpi, and an 80x80 drawable for hdpi.  If I get this Drawable object using context.getResources().getDrawable(...), it returns the 60x60 version.\n\nIs there any way to correctly deal with resources at runtime from the context of an AppWidget?\n\nThanks!",
  "lastActivityDate": "2014-04-17T19:43:47.980",
  "title": "How can I get the correct DisplayMetrics from an AppWidget in Android?",
  "tags": [
    "android",
    "resources",
    "android-widget",
    "appwidget"
  ],
  "docScore": 0,
  "comments": [],
  "answers": [],
  "creationYearMonth": "201404",
  "itemTally": 0,
  "owner": null
}